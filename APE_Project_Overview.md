# **Observability and User Interface Plan**

The strategy is built upon a consolidated **Loki/Prometheus/Grafana (LPG)** stack, deployed alongside the core services using Docker Compose, to provide centralized logging, metrics, and visualization.1

## **I. Observability Architecture: The Centralized Logging Stack**

Given the highly dynamic and ephemeral nature of the Docker container environment (where agents are constantly spun up and terminated to simulate load), a centralized logging system is mandatory to prevent data loss.

### **A. Core Components and Data Flow**

1. **Passive Logging (Llama Agents & SUT):** All containerized services (Llama Agents, MCP Gateway, Target SUT) use **Passive Logging**, simply outputting all stateful events, errors, and access details to standard output (stdout/stderr).  
2. **Promtail (Log Collection):** A lightweight agent deployed on the host system, configured to tail the Docker logs and stream all output to the centralized Loki server. This ensures that log data from transient containers is collected and moved before the container is destroyed.  
3. **Loki (Log Storage):** A highly performant log aggregation system optimized for indexing log data centrally for fast searching and correlation.2  
4. **Prometheus (Metrics Collection):** Used alongside cAdvisor and Node Exporter to gather system-level metrics (CPU, memory, container counts) and performance data from the MCP Gateway and Cerebras proxy.  
5. **Grafana (Visualization):** The single pane of glass for displaying real-time dashboards.

### **B. The Crucial Role of Correlated Logs (Traceability)**

To prove **Realism** (stateful completion), the system must be able to trace a single user session across multiple service boundaries (Agent → MCP → SUT).2

**Action Plan:**

* **Trace ID Injection:** The Llama Agent must be programmed to inject a unique **Session/Trace ID** into the HTTP headers of its first tool call (e.g., X-Trace-ID: \<UUID\>).  
* **Trace ID Persistence:** The MCP Gateway and the Target SUT must be configured to log this X-Trace-ID alongside every corresponding log entry.  
* **Log Correlation:** In Grafana/Loki, this unique ID is used to filter and visualize the entire chain of actions, errors (e.g., a 401 Unauthorized), and subsequent stateful decisions made by a single agent as it navigates a complex journey (e.g., the 8-step logon sequence described in earlier planning).4

## **II. Key Performance Indicators (KPIs) for Validation**

Validation requires metrics that prove the superiority of the AI simulation over traditional load tests. These KPIs fall into three categories:

### **A. Cognitive and Cost Metrics (Agent Performance)**

These metrics validate the performance of the Llama Agents and the efficiency of the Cerebras-powered cognitive loop.

| Metric | Source | Significance |
| :---- | :---- | :---- |
| **End-to-End Latency** (p50 / p95) | MCP/SUT Logs | Measures total time for a tool call (API request \+ LLM reasoning \+ execution). |
| **Time-to-First-Token (TTFT)** | Cerebras Proxy Logs | Measures the latency of the Cerebras inference endpoint, directly validating the **low cognitive latency** claim. |
| **Token Usage & Cost** | Cerebras Proxy Logs | Tracks input/output tokens to calculate operational costs and prove token efficiency. |
| **Tool Calls and Duration** | MCP Gateway/Loki | Tracks how often agents use specific tools (e.g., Tool\_HTTP\_POST, execute\_api\_request) and how long they take. |

### **B. Functional and Error Metrics (Realism)**

These metrics prove the quality of the simulated traffic.

| Metric | Source | Significance |  |
| :---- | :---- | :---- | :---- |
| **Successful Stateful Sessions (%)** | Llama Agent Logs | Percentage of agents that successfully complete the end-to-end multi-step journey (e.g., successful purchase, successful system logon and logoff).4 | **Primary MVP success metric.** |
| **Error Rate (4xx, 5xx)** | MCP Gateway Logs | Measures HTTP errors generated by the SUT, categorized by error type (e.g., 401, 503). |  |
| **Agent Traffic (Runs per Minute)** | Docker/Prometheus | Total number of new sessions or model calls initiated per time window. |  |

### **C. Infrastructure Metrics (Scalability)**

These metrics prove the operational capacity of the Docker Grid.5

| Metric | Source | Significance |
| :---- | :---- | :---- |
| **Concurrent Active Agents** | Prometheus/cAdvisor | Tracks the live count of llama\_agent containers, validating the high-scale load generation capability.5 |
| **CPU/Memory Utilization** | Prometheus/Node Exporter | Tracks resource consumption to identify bottlenecks in the host or the SUT. |

## **III. Presentation Layer: CLI and Grafana Dashboard**

The user interface must be immediate, intuitive, and impactful.

### **A. The Grafana Dashboard (The Visual MVP)**

The core presentation will be a dedicated Grafana Dashboard, provisioned immediately upon startup.

| Dashboard Panel | Data Source | Visualization |
| :---- | :---- | :---- |
| **Real-Time Load** | Prometheus (cAdvisor) | Gauge or Line Chart: Current count of **Concurrent Active Agents**. |
| **Success Rate** | Loki (Agent Logs) | Single Value Panel: **Successful Stateful Sessions (%)**. |
| **Core Latency** | Cerebras/MCP Metrics | Time Series: **Time-to-First-Token (TTFT)** and **End-to-End Latency** (p95). |
| **Error Breakdown** | Loki (MCP Logs) | Pie Chart: Distribution of HTTP response codes (200, 4xx, 5xx). |
| **Live Agent Trace** | Loki (All Service Logs) | Log Panel: Filterable live log feed, allowing correlation by the **Session/Trace ID**. |

### **B. The APE Command Line Interface (CLI)**

The command line is the primary control surface for the user. It abstracts the underlying Docker Compose complexity and provides simple, powerful commands for managing the test.

| CLI Action | Command | Purpose |
| :---- | :---- | :---- |
| **Setup** | `npx create-ape-load` | Initiates an interactive wizard to generate all necessary configuration files. |
| **Load Scaling** | `ape-load start --agents N` | Starts the test environment and dynamically scales the load to N concurrent agents. |
| **Live Tracing** | `ape-load logs -f mcp_gateway --grep <TRACE_ID>` | Provides a simple way to trace a specific session through the MCP Gateway. |
| **Agent Output** | `ape-load logs -f <AGENT_ID>` | Streams a specific agent's internal reasoning and tool call JSON for low-level debugging. |
| **Stop Test** | `ape-load stop` | Shuts down the entire test environment and observability stack. |

This consolidated architecture ensures the user can easily generate, observe, and validate the complexity and scale of the AI-driven traffic.